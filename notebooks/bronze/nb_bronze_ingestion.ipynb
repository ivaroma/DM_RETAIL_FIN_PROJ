{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d30339-27aa-489c-98dc-81f69092281a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0e51c1-62bf-46b2-8f18-c282e6612d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook performs the automated ingestion from the Raw landing zone to the Bronze Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52403776-a87f-4891-a3cd-5823fa0460ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3adf796-3e28-4018-bfe1-4c1609c6c0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../nb_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed06abe-cd03-491a-b852-46a604611a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 0. Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ee5869-7174-422d-86fc-2a7dc0fa34fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"entity_filter\", \"all\")\n",
    "filter_value = dbutils.widgets.get(\"entity_filter\").strip().lower()\n",
    "\n",
    "if filter_value == \"all\":\n",
    "    target_entities = \"all\"\n",
    "else:\n",
    "    target_entities = [e.strip() for e in filter_value.split(\",\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3255ce-5b1c-44b8-9d8a-ae2be78469f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. The Ingestion Map\n",
    "Mapping an entity folders to file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f08efa-1d5f-4b16-bd16-e0b586b3a26e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "ingestion_configs = [\n",
    "    {\"entity\": \"accounts\", \"format\": \"csv\"},\n",
    "    {\"entity\": \"audit_logs\", \"format\": \"avro\"},\n",
    "    {\"entity\": \"branches\", \"format\": \"csv\"},\n",
    "    {\"entity\": \"credit_scores\", \"format\": \"parquet\"},\n",
    "    {\"entity\": \"customers\", \"format\": \"json\"},\n",
    "    {\"entity\": \"exchange_rates\", \"format\": \"csv\"},\n",
    "    {\"entity\": \"transactions\", \"format\": \"csv\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05bffc8f-7a68-4773-a5ef-08ad4fa1a823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. AutoLoader Function\n",
    "A reusable function that handles the audit columns, rescued data, and archiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fdfc4e-66bc-4691-bc8a-e5e70260b189",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def ingest_raw_data(entity_name, source_format):\n",
    "    source_entity_data_path = f\"{paths['raw_data']}/{entity_name}\"\n",
    "    target_table = f\"{paths['bronze_db']}.{entity_name}\"\n",
    "    checkpoint_path = f\"{paths['checkpoints']}/{entity_name}\"\n",
    "    schema_log_path = f\"{checkpoint_path}/_schema\"\n",
    "\n",
    "    reader = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", source_format)\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "        .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "        .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "    )\n",
    "\n",
    "    if source_format.lower() == \"json\":\n",
    "        reader = reader.option(\"multiLine\", \"true\")\n",
    "\n",
    "    query = (reader.load(source_entity_data_path)\n",
    "        .withColumn(\"_bronze_ingest_ts\", current_timestamp())\n",
    "        .withColumn(\"_source_file\", col(\"_metadata.file_path\"))\n",
    "        .writeStream\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(target_table)\n",
    "    )\n",
    "    \n",
    "    query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f84802c-28fb-4aab-9f0a-66c7cf23aaa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675b23b6-12fd-4e15-bdbf-3f393e5f42e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "for config in ingestion_configs:\n",
    "    entity = config[\"entity\"]\n",
    "    \n",
    "    if target_entities == \"all\" or entity in target_entities:\n",
    "        # 1. Ingest data to Bronze\n",
    "        ingest_raw_data(\n",
    "            entity_name=entity, \n",
    "            source_format=config[\"format\"]\n",
    "        )\n",
    "        \n",
    "        # 2. Archival\n",
    "        source_dir = f\"{paths['raw_data']}/{entity}\"\n",
    "        archive_dir = f\"{paths['archive_data']}/{entity}\"\n",
    "        \n",
    "        # Create archive directory if it doesn't exist\n",
    "        dbutils.fs.mkdirs(archive_dir)\n",
    "        \n",
    "        # Move all files from source to archive\n",
    "        try:\n",
    "            files = dbutils.fs.ls(source_dir)\n",
    "            if files:\n",
    "                for f in files:\n",
    "                    dbutils.fs.mv(f.path, f\"{archive_dir}/{f.name}\")\n",
    "                print(f\"Archived {len(files)} files for {entity}\")\n",
    "        except Exception as e:\n",
    "            print(f\"No files found to archive for {entity}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29959fec-8d70-4070-8967-74f13af78ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(paths['checkpoints'], recurse=True)\n",
    "# spark.sql(f\"DROP DATABASE IF EXISTS {paths['bronze_db']} CASCADE\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4986786930776914,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_bronze_ingestion",
   "widgets": {
    "entity_filter": {
     "currentValue": "all",
     "nuid": "e050a033-d3e0-409b-bd6b-04bc995fcf59",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "customers",
      "label": null,
      "name": "entity_filter",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "customers",
      "label": null,
      "name": "entity_filter",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
