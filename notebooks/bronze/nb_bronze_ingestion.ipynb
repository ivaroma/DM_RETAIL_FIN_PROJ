{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d30339-27aa-489c-98dc-81f69092281a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0e51c1-62bf-46b2-8f18-c282e6612d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook performs the automated ingestion from the Raw landing zone to the Bronze Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52403776-a87f-4891-a3cd-5823fa0460ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3adf796-3e28-4018-bfe1-4c1609c6c0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../nb_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a4f5b7-a299-40e1-94a7-40ccd20ce218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utilities/nb_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6ee5869-7174-422d-86fc-2a7dc0fa34fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"entity_filter\", \"all\")\n",
    "filter_value = dbutils.widgets.get(\"entity_filter\").strip().lower()\n",
    "\n",
    "if filter_value == \"all\":\n",
    "    target_entities = \"all\"\n",
    "else:\n",
    "    target_entities = [e.strip() for e in filter_value.split(\",\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa3255ce-5b1c-44b8-9d8a-ae2be78469f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. The Ingestion Map\n",
    "Mapping folders to file formats and specific schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f08efa-1d5f-4b16-bd16-e0b586b3a26e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "ingestion_configs = [\n",
    "    {\"entity\": \"accounts\", \"format\": \"csv\", \"schema\": accounts_schema},\n",
    "    {\"entity\": \"audit_logs\", \"format\": \"avro\", \"schema\": audit_logs_schema},\n",
    "    {\"entity\": \"branches\", \"format\": \"csv\", \"schema\": barnches_schema},\n",
    "    {\"entity\": \"credit_scores\", \"format\": \"parquet\", \"schema\": credit_scores_schema},\n",
    "    {\"entity\": \"customers\", \"format\": \"json\", \"schema\": customers_schema},\n",
    "    {\"entity\": \"exchange_rates\", \"format\": \"csv\", \"schema\": exchange_rates_schema},\n",
    "    {\"entity\": \"transactions\", \"format\": \"csv\", \"schema\": transactions_schema}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05bffc8f-7a68-4773-a5ef-08ad4fa1a823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. AutoLoader Function\n",
    "A reusable function that handles the audit columns, rescued data, and archiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fdfc4e-66bc-4691-bc8a-e5e70260b189",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def ingest_raw_data(entity_name, source_format, source_schema):\n",
    "    source_data_path = f\"{paths['raw_data']}/{entity_name}\"\n",
    "    target_table = f\"{paths['bronze_db']}.{entity_name}\"\n",
    "    checkpoint_path = f\"{paths['checkpoints']}/{entity_name}\"\n",
    "    archive_path = f\"{paths['archive_data']}/{entity_name}\"\n",
    "    \n",
    "    (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", source_format)\n",
    "        .option(\"cloudFiles.schemaLocation\", f\"{checkpoint_path}_schema\")\n",
    "        .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "        .schema(source_schema)\n",
    "        .load(source_data_path)\n",
    "        .withColumn(\"_ingested_at\", current_timestamp())\n",
    "        .withColumn(\"_source_file\", col(\"_metadata.file_path\"))\n",
    "        .writeStream\n",
    "        .option(\"checkpointLocation\", checkpoint_path)\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(target_table)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f84802c-28fb-4aab-9f0a-66c7cf23aaa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "675b23b6-12fd-4e15-bdbf-3f393e5f42e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "for config in ingestion_configs:\n",
    "    entity = config[\"entity\"]\n",
    "    \n",
    "    if target_entities == \"all\" or entity in target_entities:\n",
    "        # 1. Ingest data to Bronze\n",
    "        ingest_raw_data(\n",
    "            entity_name=entity, \n",
    "            source_format=config[\"format\"], \n",
    "            source_schema=config[\"schema\"]\n",
    "        )\n",
    "        \n",
    "        # 2. Archival\n",
    "        source_dir = f\"{paths['raw_data']}/{entity}\"\n",
    "        archive_dir = f\"{paths['archive_data']}/{entity}\"\n",
    "        \n",
    "        # Create archive directory if it doesn't exist\n",
    "        dbutils.fs.mkdirs(archive_dir)\n",
    "        \n",
    "        # Move all files from source to archive\n",
    "        try:\n",
    "            files = dbutils.fs.ls(source_dir)\n",
    "            if files:\n",
    "                for f in files:\n",
    "                    dbutils.fs.mv(f.path, f\"{archive_dir}/{f.name}\")\n",
    "                print(f\"Archived {len(files)} files for {entity}\")\n",
    "        except Exception as e:\n",
    "            print(f\"No files found to archive for {entity}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8926187378598539,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
